---
alias: KL-Divergence
---

$$H(P,Q) - H(P) = \underbracket{-\sum_{x \in X} p(x)\log q(x)}_{\text{cross-entropy}} - \underbracket{\big(-\sum_{x \in X} p(x)\log p(x)\big)}_{\text{entropy}}$$