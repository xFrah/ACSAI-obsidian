Information gain or **IG** measures how well a given attribute separates the training examples according to their target classification. 

Constructing a decision tree is all about finding an attribute that returns the highest information gain and the smallest [entropy](Entropy.md).

![](../z_images/Pasted%20image%2020230519115645.png)


**Information gain is a decrease in entropy.** It computes the difference between entropy before split and average entropy after split.

Mathematically, IG is represented as:

$$\large \text{Information Gain}=\text{Entropy(before)} - \sum^{K}_{j=0} \text{Entropy(j, after)}$$

Where:
- “before” is the dataset before the split, 
- K is the number of subsets generated by the split,
- (j, after) is subset j after the split.