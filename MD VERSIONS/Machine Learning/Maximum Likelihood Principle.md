## The "useless" bayesian vision

The goal of maximum likelihood is to fit a distribution to some data.
Using the [Bayes Theorem](Bayes%20Theorem.md), we want to find the most likely value for the parametetrs of our model, given the data.

$$\large argmax\,P(\theta \;|\; X) = argmax \; \frac{P(X\; | \; \theta)P(\theta)}{P(X)}$$
Where:
- $P(\theta)$ is called the prior
- $P(X \; | \; \theta)$ is called the likelihood, which is not really a probability
- $P(\theta \; | \; X)$ is called the posterior

The likelyhood is equal to the probability density function of a gaussian, if we assume that the data was generated by a [gaussian distribution](Gaussian%20distribution.md).


## Now the real stuff lol

We basically bruteforce fit a gaussian distribution on the data and then we get the one that maximizes the likelyhood function.

So we begin by fitting the gaussians, we start with $\mu = 28$ and $\sigma = 2$:

![](../z_images/Pasted%20image%2020230414000844.png)

Which yields a likelyhood of:
$$\large L({\color{green} \boldsymbol{\mu}, \boldsymbol{\Sigma}};x) = \frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}}^{\LARGE-\frac{1}{2}({\mathbf{x}}-{\color{green}\boldsymbol{\mu}})^{\mathrm{T}}{\color{green} \boldsymbol{\Sigma}^{-1}}({\mathbf{x}}-{\color{green}\boldsymbol{\mu}})} = 0.03$$

![](../z_images/Pasted%20image%2020230414001222.png)

But we can do better right?
In fact, if we plug in $\mu = 30$ and $\sigma = 2$:

![](../z_images/Pasted%20image%2020230414001424.png)

We get a likelyhood of 0.12, which is definitely better!

By the way, if we plot the likelyhood hovering all over the possible values of $\mu$, we can actually see that we can get the maximum likelyhood where the derivative of the whole thing is 0:

![](../z_images/Pasted%20image%2020230414001747.png)


If we have multiple data points, the likelihood function will be the product of all the gaussians/individual likelyhood functions that are generated from the data points.

$$\large p({\color{red}{D}};\boldsymbol{\mu}, \boldsymbol{\Sigma}) = p({\color{red}\{x_1,\ldots,x_N \}};\boldsymbol{\mu}, \boldsymbol{\Sigma})= p({\color{red}x_1};\boldsymbol{\mu}, \boldsymbol{\Sigma})\cdot p({\color{red}x_2};\boldsymbol{\mu}, \boldsymbol{\Sigma})\ldots \cdot p({\color{red}x_N};\boldsymbol{\mu}, \boldsymbol{\Sigma})$$$$\large p({\color{red}D};\boldsymbol{\mu}, \boldsymbol{\Sigma}) = \prod_{i=1}^N p({\color{red}x_i};\boldsymbol{\mu}, \boldsymbol{\Sigma}) = \prod_{i=1}^N  \mathcal{N}({\color{red}\mathbf{x}_i}; \boldsymbol{\mu}, \boldsymbol{\Sigma})$$
![](../z_images/Pasted%20image%2020230414002635.png)

So we take the derivative of this shit with respect to $\mu$ and we actually find the Maximum Likelihood parameters.

Obviously, you can do the same to find the standard deviations. You lock in the $\mu$ and you let the [standard deviation](../Statistics/Standard%20Deviation.md) change, then you stick with the value that gives the maximum likelyhood:

![](../z_images/Pasted%20image%2020230415125133.png)


```ad-tldr
In order to get the maximum likelihood parameters for multiple data points, we must multiply all the individual likelihood functions and take the derivative of that, solving for $\mu$ and $\sigma$.
```


## After endless mathematical proofs... the end

There is a whole ass proof to justify that the maximum likelihood estimate is equal to the [mean](../Statistics/Mean.md) of the measurements. The proof is covered in the link below:

![](../z_images/Pasted%20image%2020230415124246.png)

Similarly, there is a proof that shows that the width of the distribution is equal to the [standard deviation](../Statistics/Standard%20Deviation.md) of the measurements:

![](../z_images/Pasted%20image%2020230415124322.png)

These notions may be obvious, but now we have the math to back it up.

```ad-tldr
![](../z_images/Pasted%20image%2020230415124450.png)

<br>

![](../z_images/Pasted%20image%2020230415124511.png)
```


```ad-seealso
title: Links
https://youtu.be/Dn6b9fCIUpM
```
